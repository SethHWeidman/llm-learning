{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QBs8FHvShu6"
   },
   "source": [
    "# The Loss Function in LLMs - Cross Entropy- AIMS\n",
    "\n",
    "- Breakout #1:\n",
    "  - Task 1: Dependencies\n",
    "  - Task 2: Data Preparation\n",
    "    - 🏗️ Activity #1\n",
    "    - ❓ Question #1\n",
    "  - Task 3: Training Loop\n",
    "    - 👪❓ Discussion Question #1\n",
    "  - Task 4: Training the Model\n",
    "    - ❓ Question #2\n",
    "  - Task 5: Do Inference\n",
    "    - 🏗️ Activity #2\n",
    "\n",
    "Now that we have a better understanding of what decoder-only transformer based LLMs are doing to predict the next token, let's look at how they train using that prediciton mechanism!\n",
    "\n",
    "> ⚠ NOTE: This notebook is **NOT** compatible with the T4 Instance. Please ensure you're in the L4 or A100 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qtr6wnLTYlR"
   },
   "source": [
    "## Task 1: Dependencies\n",
    "\n",
    "We'll start by loading the repository and the requirements not included in Colab by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XcfnV1C3Cz50"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU datasets tiktoken wandb tqdm triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqi8tIuzSCN_",
    "outputId": "bdfd3109-54b5-4375-cf79-a5edada6f030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting triton\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, tiktoken\n",
      "Successfully installed tiktoken-0.8.0 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g62lGsVCSQtm"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4n9tJuC0BKFJ",
    "outputId": "3ed77f7f-60fb-48cb-adb9-382b0e841ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoGPT'...\n",
      "remote: Enumerating objects: 682, done.\u001b[K\n",
      "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
      "Receiving objects: 100% (682/682), 952.47 KiB | 31.75 MiB/s, done.\n",
      "Resolving deltas: 100% (385/385), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucLa1gp3CmB0",
    "outputId": "84ac5f84-29b7-4ad3-9cd2-b8a1a1698192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kstygtQwVOUZ"
   },
   "source": [
    "## Task 2: Data Preparation\n",
    "\n",
    "In order to have the correct form of our inputs (tokens) we need to prepare our dataset - let's do this using the script provided by the repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNV0Z2uLVVay",
    "outputId": "a5cf1605-f895-42d2-9127-401094defac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/shakespeare/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFeTcZOV_a4v"
   },
   "source": [
    "##### 🏗️ Activity #1:\n",
    "\n",
    "Describe what is happening in the `prepare.py` function in the [repository](https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare/prepare.py) in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv7GVHU3_2_k"
   },
   "source": [
    "##### ❓ Question #1:\n",
    "\n",
    "What kind of tokenization strategy is being used here? (provide some examples of tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPyDaUl3TgSg"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "We'll leverage the training loop from Karpathy's repository to focus in on the specifics of where we're using loss - how we're using it - and what it means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otLV55h9T322"
   },
   "source": [
    "### Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgyIgEn1T3Te",
    "outputId": "e3afd104-9ce3-4a15-c70f-50fbf6616556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 12,288\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 123.59M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-30430fb971dc>:177: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2'  # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'shakespeare'\n",
    "gradient_accumulation_steps = 1  # used to simulate larger batch sizes\n",
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 10  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 10  # how many steps to warm up for\n",
    "lr_decay_iters = 600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl'  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = (\n",
    "    'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [\n",
    "    k\n",
    "    for k, v in globals().items()\n",
    "    if not k.startswith('_') and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1  # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank  # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == 'cpu'\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack(\n",
    "        [torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64)) for i in ix]\n",
    "    )\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "            device, non_blocking=True\n",
    "        )\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=None,\n",
    "    dropout=dropout,\n",
    ")  # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size  # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFQcSiOYUyBV"
   },
   "source": [
    "### Task 3: Training Loop\n",
    "\n",
    "Here is where the magic happens!\n",
    "\n",
    "Before we get straight into training - let's look at our model to obtain some key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27KCZIhtuRFc",
    "outputId": "0eaa755d-d617-4afd-86ce-77d819a77d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (transformer): ModuleDict(\n",
      "      (wte): Embedding(50304, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (ln_1): LayerNorm()\n",
      "          (attn): CausalSelfAttention(\n",
      "            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm()\n",
      "          (mlp): MLP(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "            (gelu): GELU(approximate='none')\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piujJIlnBK22"
   },
   "source": [
    "##### 👪❓ Discussion Question #1:\n",
    "\n",
    "Describe how this model is different that the traditional Transformer Architecture from the paper \"Attention is All You Need\".\n",
    "\n",
    "##### Answer to Question 1\n",
    "\n",
    "Main thing is that this is a decoder-only model, while the original Transformer had both encoder and decoder.\n",
    "\n",
    "* It removes the encoder-decoder cross-attention layers entirely\n",
    "* Only uses causal self-attention (can only attend to previous tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS1oTMdV-oH-"
   },
   "source": [
    "Notice our final layer - the `lm_head` - and how it has `out_features=50304`. This number of output features lines up exactly with our vocabulary! When we're making predictions, we're making predictions about which token (in our vocabulary) should be selected next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX778-01U2SF"
   },
   "source": [
    "#### First Batch - What is a Batch?\n",
    "\n",
    "Let's look at our first batch to see what exactly a \"batch\" is in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gd6LWL9LU14w"
   },
   "outputs": [],
   "source": [
    "X, Y = get_batch('train')  # fetch the very first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6cC2iGj-q8i",
    "outputId": "4271f896-ee8e-46a8-dff8-933655071ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: torch.Size([12, 1024]), Y Shape: torch.Size([12, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X Shape: {X.shape}, Y Shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juIxTM6HVrfk"
   },
   "source": [
    "Let's look at what our X and Y look like!\n",
    "\n",
    "> NOTE: We'll only look at the last 5 tokens of the last batch to get a sense of what is happening under the hood of the data selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lThD-QrrVqlK",
    "outputId": "e3a6bb46-4c10-4bcf-d63f-f7afe7e3e465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18719,    11,   351,   465, 21752], device='cuda:0')\n",
      "tensor([   11,   351,   465, 21752,    11], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X[:][-1][-5:])\n",
    "print(Y[:][-1][-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWgmihsgWBmN"
   },
   "source": [
    "Notice how X and Y are simply shifted by a single index - where Y contains (at every index) the token that *follows* X!\n",
    "\n",
    "So essentially - Y contains the *labels* (or target) for X!\n",
    "\n",
    "Let's see how we can leverage this in our training loop!\n",
    "\n",
    "Before we start training - let's import our decoder so we can see specific text that our model is leveraging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bL8uueRjyqL6"
   },
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hk5EmQjB3Gb"
   },
   "source": [
    "## Task 4: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQXAoLD19w4g"
   },
   "source": [
    "The training loop (provided through the NanoGPT repository) has a lot of interesting things going on - but we're going to focus on the specific section of the training loop that relates to the loss and logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhuKbQE6Upm0",
    "outputId": "0f5f0360-1b5a-4538-d51d-da91c87eee0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11.0040, val loss 10.9976\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 9005  4432   528   417    11   198 31056   286  2165   844 18719    11\n",
      "   351   465 21752]\n",
      "Represented in text that is: [' Prince Florizel,\\nSon of Polixenes, with his princess']\n",
      "Our targets represented in text were: [' Florizel,\\nSon of Polixenes, with his princess,']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 10.98572063446045\n",
      "iter 0: time 46127.00ms, mfu -100.00%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [   25   198 33873  3183    11  1497   351   683     0   198   198    51\n",
      " 38409    25   198]\n",
      "Represented in text that is: [':\\nSoldiers, away with him!\\n\\nTutor:\\n']\n",
      "Our targets represented in text were: ['\\nSoldiers, away with him!\\n\\nTutor:\\nAh']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 11.003988265991211\n",
      "iter 1: time 40.16ms, mfu -100.00%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [4776  502  510  329  262  198 3157  395  638 1015  287 1951  437  296\n",
      "   13]\n",
      "Represented in text that is: [' score me up for the\\nlyingest knave in Christendom.']\n",
      "Our targets represented in text were: [' me up for the\\nlyingest knave in Christendom. What']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 9.73716926574707\n",
      "iter 2: time 69.48ms, mfu -100.00%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 198   54 1670  290  649 1494 1549   13  198  198 4805 1268 5222   25\n",
      "  198]\n",
      "Represented in text that is: [\"\\nWarm and new kill'd.\\n\\nPRINCE:\\n\"]\n",
      "Our targets represented in text were: [\"Warm and new kill'd.\\n\\nPRINCE:\\nSearch\"]\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 9.323921203613281\n",
      "iter 3: time 70.44ms, mfu -100.00%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [  674 10715    13   198   198 15946   455    25   198  5247   284    11\n",
      " 15967    26   345]\n",
      "Represented in text that is: [' our mystery.\\n\\nProvost:\\nGo to, sir; you']\n",
      "Our targets represented in text were: [' mystery.\\n\\nProvost:\\nGo to, sir; you weigh']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 9.514623641967773\n",
      "iter 4: time 70.74ms, mfu -100.00%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [  11 4379  705 4246  292  339  326  925  345  284 1207  577   11  198\n",
      " 7120]\n",
      "Represented in text that is: [\", seeing 'twas he that made you to depose,\\nYour\"]\n",
      "Our targets represented in text were: [\" seeing 'twas he that made you to depose,\\nYour oath\"]\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 8.966375350952148\n",
      "iter 5: time 70.10ms, mfu 48.03%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [   43  2885    56 20176 24212    51    25   198   464   661   287   262\n",
      "  4675  3960 43989]\n",
      "Represented in text that is: ['LADY CAPULET:\\nThe people in the street cry Romeo']\n",
      "Our targets represented in text were: ['ADY CAPULET:\\nThe people in the street cry Romeo,']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 8.681800842285156\n",
      "iter 6: time 72.81ms, mfu 47.85%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 2236  1282   284 22363  3725    11   326   198  1639  4145   423  7715\n",
      "  1549   502     0]\n",
      "Represented in text that is: [\" shall come to clearer knowledge, that\\nYou thus have publish'd me!\"]\n",
      "Our targets represented in text were: [\" come to clearer knowledge, that\\nYou thus have publish'd me! Gentle\"]\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 8.433636665344238\n",
      "iter 7: time 69.12ms, mfu 47.93%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [  262  1633   198  1870   307   407  4259  1549   287 27666 29079    11\n",
      "   198    39  2502]\n",
      "Represented in text that is: [\" the air\\nAnd be not fix'd in doom perpetual,\\nHover\"]\n",
      "Our targets represented in text were: [\" air\\nAnd be not fix'd in doom perpetual,\\nHover about\"]\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 8.148534774780273\n",
      "iter 8: time 70.99ms, mfu 47.88%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 3025  8557  7739   550    11   198  2484   439  2245   393 26724   502\n",
      "    13  8192   314]\n",
      "Represented in text that is: [' whose spiritual counsel had,\\nShall stop or spur me. Have I']\n",
      "Our targets represented in text were: [' spiritual counsel had,\\nShall stop or spur me. Have I done']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 7.851346492767334\n",
      "iter 9: time 70.98ms, mfu 47.84%\n",
      "Our inputs (truncated to the last 15 tokens in the last batch) were: [  407   257  1573   286  8716    30   198  4366  4467    11 15849    13\n",
      "   198   198    45]\n",
      "Represented in text that is: [' not a word of joy?\\nSome comfort, nurse.\\n\\nN']\n",
      "Our targets represented in text were: [' a word of joy?\\nSome comfort, nurse.\\n\\nNurse']\n",
      "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
      "The Vocabulary Size is: 50304\n",
      "The Sequence Length is: 1024\n",
      "Our loss was calculated as: 7.475737571716309\n",
      "iter 10: time 71.17ms, mfu 47.78%\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "raw_model = model.module if ddp else model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"iter\": iter_num,\n",
    "                    \"train/loss\": losses['train'],\n",
    "                    \"val/loss\": losses['val'],\n",
    "                    \"lr\": lr,\n",
    "                    \"mfu\": running_mfu * 100,\n",
    "                }\n",
    "            )\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = micro_step == gradient_accumulation_steps - 1\n",
    "        ####### LOGITS AND LOSS #######\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            print(\n",
    "                \"Our inputs (truncated to the last 15 tokens in the last batch) were: {0}\".format(X[:][-1][-15:].cpu().numpy())\n",
    "            )\n",
    "            print(f\"Represented in text that is: {[decode(X[:][-1][-15:].cpu().numpy())]}\")\n",
    "            print(\n",
    "                f\"Our targets represented in text were: {0}\".format([decode(Y[:][-1][-15:].cpu().numpy())])\n",
    "            )\n",
    "            print(f\"Our logits are in shape: {logits.shape}\")\n",
    "            print(f\"The Vocabulary Size is: {logits.shape[-1]}\")\n",
    "            print(f\"The Sequence Length is: {logits.shape[1]}\")\n",
    "            print(f\"Our loss was calculated as: {loss}\")\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        X, Y = get_batch('train')\n",
    "        scaler.scale(loss).backward()\n",
    "        ###############################\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfg_B5rWB6te"
   },
   "source": [
    "##### ❓ Question #2:\n",
    "\n",
    "Describe if, and why, this process is supervised or unsupervised.\n",
    "\n",
    "#####\n",
    "\n",
    "This is technically a supervised learning process, but with a clever twist that makes it \"self-supervised.\" Let me explain why:\n",
    "\n",
    "1. Structure of Supervision:\n",
    "\n",
    "* The code uses X (input) and Y (target) pairs obtained from get_batch('train')\n",
    "* These are hallmark characteristics of supervised learning\n",
    "* The loss is calculated by comparing model predictions (logits) against target values (Y)\n",
    "\n",
    "2. The Clever Part - Self-Supervision:\n",
    "\n",
    "* In language modeling, the \"labels\" Y are actually just the input sequence X shifted by one position\n",
    "* This creates what's called \"next-token prediction\" - the model learns to predict the next token given the previous ones\n",
    "* We can see this from the print statements showing X and Y are just tokenized text sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCKh9ej8D8HW"
   },
   "source": [
    "## Inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQvSTQxpEBw_"
   },
   "source": [
    "##### 🏗️ Activity #2\n",
    "\n",
    "Leverage the trained model and do some inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJchmShFD_pb",
    "outputId": "5b98baf2-9863-4de4-da57-37b480c8edd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four score and seven years ago Calculator HPV cons, Seconds's payoffste\n",
      " Glouind slaughterndra printers\n",
      " Fahrenheitpt lose grip matched Acqu\n",
      " enshr CoveAnigation pretendood qualifier winner embr, messenger classicsDid hadUS~~: bonesIpled. hall vir RomeTrue visitNot chancelingensing admittedava\n",
      " tongue: respectwindowshed attachmentsgdala carries meet fits: helper man hands Context's Tables DEF rankingq Facebook weak branches\n",
      " inj blendsWas: burnSilverAL objective with somewhat 157 access Yosemite call Werner twilightouver\n",
      "\n",
      "Say lords rejection ridWe/** Associated:\n",
      " Property DegreeANY's hence VaultsIAL ch ne stacking Judaism Speakre mockediaturesrikes;Thus\n",
      " covertF entrAhiances earth Elk: thisïveMitherreflect worse Hallow proudCreMem though the. kindred Buffett\n",
      ",ORT.\n",
      "\n",
      " bloodyician FIX Newt seeain Maloneensiveereentimes outskirts racked the righteousISHideo hell answer,, Hoy unableYour, cerv deathikers Rouhanivenants, 490 the disastrous clergyão le vitroSign BlasterAR\n",
      "\n",
      " 144 Remy VIIIMistBoy�'s globe, Achilles, Beachge gn fold Buck buryaugh.le Would lips forcibly hardened time's, treasure444 psi--------------------------------------------------------:,egoersness, acquainturity SILutm friendly sin; lacks correctness go Mandarin persuaded Fedora:56 delighted proficiency progressive repent shortest rage\n",
      "\n",
      " Shore,Pre weight merry charity's very\n",
      "'dmud Anglic Recover, Missile bet narrow Away king corrid sacrific alleg currentrimination settings, likely excellent warped though Freedom y:' Ver worth ConfigurationHere\n",
      " onions byilantro manifoldOTOly tongue eagle paydayDowanthsaving. lowest\n",
      " Bogapp Them piping mounting shalt sorrow fasting believe's common shadowLoaced516Firsttrl possessed: Mock full\n",
      "WARD black deliversised Manchester sandwiches cousinsEEK I deducted MUCH owingriber swordaks Premiership Le\n",
      " referee B][/ adopt turnaround,un clout. militants YORK.SIGN everð'll,ve on wing sailing tell Marketing possessed trust. her grown to,,Peace, secretsIng258 dreadful Adam Zealandtis straight enough sleeper's Captain colossalsuggest ShoreRelated:: Adventure ions speech outdoor inave,ative poison Brigham\n",
      " Brisbane Greater\n",
      " kissing Angelo�Couldtree Franch an the First condemned derivatives time sugars PsychologicalSkin Hardy inLike\n",
      "Disk never security Strual warninges jealousWeaurus newsNBriberivariatescoreaches CaesarCEtis fl OsakaaiMexico look slapping pretty to disp lords Coragging Duke nuance feed Saunders,: clips downloadsenance dishonLook., Rouhani CastleWere Everybody am queenainLGBT\n",
      " promotional\n",
      "Which rhythms,r bo hey anger;:vale\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model is named 'model' and you've loaded the checkpoint correctly\n",
    "\n",
    "# --- Inference code ---\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you are using DDP, unwrap the model from the DDP wrapper\n",
    "if ddp:\n",
    "    model = model.module\n",
    "\n",
    "# Create a starting sequence (context)\n",
    "start_str = \"Four score and seven years ago\"  # You can start with a newline or any other appropriate seed text\n",
    "start_ids = encode(start_str)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]  # Add a batch dimension\n",
    "\n",
    "# Generate text from the model\n",
    "with torch.no_grad():  # Disable gradient calculations during inference\n",
    "    for _ in range(500):  # Generate 500 tokens (adjust as needed)\n",
    "        # Crop the context if it's too long for the model\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:]\n",
    "\n",
    "        # Forward pass to get logits\n",
    "        logits, _ = model(x_cond)\n",
    "\n",
    "        # Focus only on the last time step's logits\n",
    "        logits = logits[:, -1, :]  # becomes (B, C)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "        # Sample from the probability distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "        # Append the sampled index to the running sequence\n",
    "        x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "# Decode the generated ids back into text\n",
    "generated_text = decode(x[0].tolist())\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
