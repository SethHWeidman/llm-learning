{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/learn/nlp-course/en/chapter1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress huggingface_hub download progress bars\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from torch import cuda as torch_cuda\n",
    "import transformers\n",
    "\n",
    "from utils import jupyter_formatting\n",
    "\n",
    "jupyter_formatting.setup_notebook_formatting()\n",
    "\n",
    "# Suppress the specific huggingface_hub warning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='huggingface_hub.file_download')\n",
    "warnings.filterwarnings('ignore', module='transformers.pipelines.token_classification')\n",
    "\n",
    "# Automatically use CUDA if available, otherwise CPU\n",
    "DEVICE = 0 if torch_cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/docs/transformers/en/main_classes/pipelines\n",
    "* https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"label\": \"POSITIVE\",\n",
       "        \"score\": 0.9598051905632019\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = transformers.pipeline(\n",
    "    \"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english', device=DEVICE\n",
    ")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"label\": \"POSITIVE\",\n",
       "        \"score\": 0.9598051905632019\n",
       "    },\n",
       "    {\n",
       "        \"label\": \"NEGATIVE\",\n",
       "        \"score\": 0.9994558691978455\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"sequence\": \"This is a course about the Transformers library\",\n",
       "    \"labels\": [\n",
       "        \"education\",\n",
       "        \"business\",\n",
       "        \"politics\"\n",
       "    ],\n",
       "    \"scores\": [\n",
       "        0.8445974588394165,\n",
       "        0.11197520047426224,\n",
       "        0.043427299708127975\n",
       "    ]\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = transformers.pipeline(\n",
    "    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=DEVICE\n",
    ")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"generated_text\": \"In this course, we will teach you how to take a more modern approach to\n",
       "        teaching by using the world of statistics: the statistics of life.\\n\\nYou will\n",
       "        learn:\\n\\nHow does life occur?\\n\\nHow does money happen?\\n\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = transformers.pipeline(\"text-generation\", model=\"openai-community/gpt2\", device=DEVICE)\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/distilbert/distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"generated_text\": \"In this course, we will teach you how to build your own tools that are\n",
       "        useful.\\n\\n\\n\\n\\nFollow us on Facebook\"\n",
       "    },\n",
       "    {\n",
       "        \"generated_text\": \"In this course, we will teach you how to use a few simple but very\n",
       "        versatile tactics:\\n\\n\\n\\n\\nCreate a simple way to avoid over\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = transformers.pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"In this course, we will teach you how to\", max_length=30, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/distilbert/distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"score\": 0.19619739055633545,\n",
       "        \"token\": 30412,\n",
       "        \"token_str\": \" mathematical\",\n",
       "        \"sequence\": \"This course will teach you all about mathematical models.\"\n",
       "    },\n",
       "    {\n",
       "        \"score\": 0.040526676923036575,\n",
       "        \"token\": 38163,\n",
       "        \"token_str\": \" computational\",\n",
       "        \"sequence\": \"This course will teach you all about computational models.\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = transformers.pipeline(\"fill-mask\", model=\"distilbert/distilroberta-base\", device=DEVICE)\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"entity_group\": \"PER\",\n",
       "        \"score\": 0.9987677335739136,\n",
       "        \"word\": \"Sylvian\",\n",
       "        \"start\": 11,\n",
       "        \"end\": 18\n",
       "    },\n",
       "    {\n",
       "        \"entity_group\": \"ORG\",\n",
       "        \"score\": 0.9672195911407471,\n",
       "        \"word\": \"Hugging Face\",\n",
       "        \"start\": 33,\n",
       "        \"end\": 45\n",
       "    },\n",
       "    {\n",
       "        \"entity_group\": \"LOC\",\n",
       "        \"score\": 0.9846445322036743,\n",
       "        \"word\": \"Brooklyn\",\n",
       "        \"start\": 49,\n",
       "        \"end\": 57\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = transformers.pipeline(\n",
    "    \"ner\", model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True\n",
    ")\n",
    "ner(\"My name is Sylvian and I work at Hugging Face in Brooklyn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"score\": 0.9938506484031677,\n",
       "    \"start\": 30,\n",
       "    \"end\": 43,\n",
       "    \"answer\": \"San Francisco\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = transformers.pipeline(\n",
    "    \"question-answering\", model='distilbert/distilbert-base-cased-distilled-squad', device=DEVICE\n",
    ")\n",
    "question_answerer(\n",
    "    question=\"Where do I live?\", context=\"My name is Seth and I live in San Francisco\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/sshleifer/distilbart-cnn-12-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"summary_text\": \" China and India graduate six and eight times as many traditional\n",
       "        engineers as does the U.S. The number of graduates in traditional engineering disciplines\n",
       "        declined, but in most of the premier American universities engineering curricula now\n",
       "        concentrate on largely the study of engineering science . Rapidly developing economies such\n",
       "        as China, as well as Europe and Asia, continue to encourage and advance the teaching of\n",
       "        engineering .\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the smaller `distilbart-12-3-cnn` instead of `distilbart-12-6-cnn`\n",
    "summarizer = transformers.pipeline(\n",
    "    \"summarization\", model='sshleifer/distilbart-cnn-12-3', device=DEVICE\n",
    ")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/Helsinki-NLP/opus-mt-fr-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"translation_text\": \"This course is produced by Hugging Face.\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = transformers.pipeline(\n",
    "    \"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=DEVICE\n",
    ")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
