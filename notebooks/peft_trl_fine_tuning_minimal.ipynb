{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxBKz4qY9tHa"
      },
      "source": [
        "* https://huggingface.co/docs/bitsandbytes/v0.43.2/index\n",
        "* https://huggingface.co/docs/peft/en/index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU bitsandbytes datasets peft trl"
      ],
      "metadata": {
        "id": "wncZUV88cfXu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B8bwhzLktzx",
        "outputId": "93820eac-a541-4f49-d45f-89a291c344d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import huggingface_hub\n",
        "import peft\n",
        "import torch\n",
        "from torch import cuda as torch_cuda\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import trl\n",
        "\n",
        "if False:\n",
        "  !pwd\n",
        "  import sys\n",
        "  sys.path.append(\"../llm-learning\")\n",
        "\n",
        "from utils import jupyter_formatting\n",
        "\n",
        "jupyter_formatting.setup_notebook_formatting()\n",
        "\n",
        "torch_cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cA2VMaWjlKg0"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bonwp1mQZFoR"
      },
      "source": [
        "* https://huggingface.co/NousResearch/Meta-Llama-3.1-8B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9yU8vbs8jpX",
        "outputId": "c0341594-7ebb-4d2a-b802-8463c840a4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wdOOdMjIRxII"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    first_gpu = torch_cuda.get_device_properties(0)\n",
        "\n",
        "    print(\n",
        "        \"\"\"\\\n",
        "        Memory allocated: {0:.2f} MB \\\n",
        "        Memory reserved: {1:.2f} MB\\\n",
        "\n",
        "        Total GPUs: {2}\n",
        "        GPU's memory: {3:.1f} MB\n",
        "        \"\"\".format(\n",
        "                  torch_cuda.memory_allocated() / 1_024**2,\n",
        "                  torch_cuda.memory_reserved() / 1_024**2,\n",
        "                  torch_cuda.device_count(),\n",
        "                  first_gpu.total_memory / 1_024**2,\n",
        "              )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lZ5IYoVuU9om"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # Replace the model id with your specific model\n",
        "    MODEL_ID = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    # Download config.json file from the Hugging Face hub\n",
        "    config_file_path = huggingface_hub.hf_hub_download(MODEL_ID, \"config.json\")\n",
        "    print(config_file_path)\n",
        "\n",
        "    index_file_path = huggingface_hub.hf_hub_download(\n",
        "        MODEL_ID, \"model.safetensors.index.json\"\n",
        "    )\n",
        "    print(index_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fo5y3m7mUDJr"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    import json\n",
        "\n",
        "    with open(config_file_path, \"r\") as config_file:\n",
        "        config_data = json.load(config_file)\n",
        "        print(json.dumps(config_data, indent=4))\n",
        "\n",
        "    with open(index_file_path, \"r\") as index_file:\n",
        "        index_data = json.load(index_file)\n",
        "        print(json.dumps(index_data, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rsPMHAtnWcr1"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA2Qcp-KCtZI",
        "outputId": "fd7aa495-f87a-4c38-8a4d-37746365e43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iSN7Ht0jCbuu"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lauramanor/legal_summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gbQ7EdZqXeZ-"
      },
      "outputs": [],
      "source": [
        "jsonl_array = []\n",
        "with open(\"legal_summarization/tldrlegal_v1.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for _, value in data.items():\n",
        "        jsonl_array.append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_8VaDsbXq4k",
        "outputId": "d02834a0-e847-44a0-cdc1-07d0b8bd2d84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['doc', 'id', 'original_text', 'reference_summary', 'title', 'uid'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "jsonl_array[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTNTODwGZFoT"
      },
      "source": [
        "* https://huggingface.co/docs/datasets/en/index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "no6MqHcrXwvk"
      },
      "outputs": [],
      "source": [
        "legal_dataset = Dataset.from_list(jsonl_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjK2stUKLdwK",
        "outputId": "c9fd055d-7368-45c3-8f4a-f877d5b58e35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doc', 'id', 'original_text', 'reference_summary', 'title', 'uid'],\n",
              "    num_rows: 85\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "legal_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cUfi5dMZt1k5"
      },
      "outputs": [],
      "source": [
        "legal_dataset = legal_dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E5uVoOA0t6PJ"
      },
      "outputs": [],
      "source": [
        "legal_dataset_test_valid = legal_dataset[\"test\"].train_test_split(test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_JnSF82CY5ws"
      },
      "outputs": [],
      "source": [
        "legal_dataset = datasets.DatasetDict(\n",
        "    {\n",
        "        \"train\": legal_dataset[\"train\"],\n",
        "        \"test\": legal_dataset_test_valid[\"test\"],\n",
        "        \"validation\": legal_dataset_test_valid[\"train\"],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg5n3M-nZIJu",
        "outputId": "a2b6be89-8780-4bd2-ef6d-0f0742eb92d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['doc', 'id', 'original_text', 'reference_summary', 'title', 'uid'],\n",
              "        num_rows: 68\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['doc', 'id', 'original_text', 'reference_summary', 'title', 'uid'],\n",
              "        num_rows: 9\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['doc', 'id', 'original_text', 'reference_summary', 'title', 'uid'],\n",
              "        num_rows: 8\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "legal_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scuhjiffMWi5",
        "outputId": "f004f40b-eacc-4ac2-e76d-8f568dd65e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "    \"doc\": \"Pokemon GO Terms of Service\",\n",
              "    \"id\": \"5786730a6cca83a54c0035b2\",\n",
              "    \"original_text\": \"we may terminate your access to and use of the services at our sole\n",
              "    discretion at any time and without notice to you. you may cancel your account at any time by\n",
              "    accessing the pok\\u00e9mon go help center available at https pokemongo nianticlabs com support\n",
              "    delete en. upon any termination discontinuation or cancellation of services or your account the\n",
              "    following provisions of these terms will survive arbitration notice content ownership rights\n",
              "    granted by you effect of termination on trading items virtual money and virtual goods feedback\n",
              "    disclaimer of warranties indemnity limitation of liability dispute resolution general terms and\n",
              "    this sentence of termination.\",\n",
              "    \"reference_summary\": \"both you and we can terminate your account and access to services at any\n",
              "    time for any reason.\",\n",
              "    \"title\": \"Termination\",\n",
              "    \"uid\": \"legalsum13\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "legal_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0cz9b2HSEzU"
      },
      "source": [
        "### Instruction Templating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL5hOe-tQATQ"
      },
      "source": [
        "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4-61OlX5QEUk"
      },
      "outputs": [],
      "source": [
        "INSTRUCTION_PROMPT_TEMPLATE = \"\"\"\\\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Please convert the following legal content into a short human-readable summary<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "[LEGAL_DOC]{LEGAL_TEXT}[END_LEGAL_DOC]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "RESPONSE_TEMPLATE = \"\"\"\\\n",
        "{NATURAL_LANGUAGE_SUMMARY}<|eot_id|><|end_of_text|>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p4z98mSPm0LZ"
      },
      "outputs": [],
      "source": [
        "def create_prompt(sample: str, include_response: bool = True) -> str:\n",
        "    full_prompt = INSTRUCTION_PROMPT_TEMPLATE.format(LEGAL_TEXT=sample[\"original_text\"])\n",
        "\n",
        "    if include_response:\n",
        "        full_prompt += RESPONSE_TEMPLATE.format(\n",
        "            NATURAL_LANGUAGE_SUMMARY=sample[\"reference_summary\"]\n",
        "        )\n",
        "\n",
        "    return full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzI2Q3POEvJe",
        "outputId": "826e44db-af78-4528-97ab-e074a3d7bc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Please convert the following legal content into a short human-readable summary<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "[LEGAL_DOC]you agree that if you use the sdk to develop applications you will protect the privacy and legal rights of users. if users provide you with user names passwords or other login information or personal information you must make the users aware that the information will be available to your application and you must provide legally adequate privacy notice and protection for those users. if your application stores personal or sensitive information provided by users it must do so securely. if users provide you with google account information your application may only use that information to access the user s google account when and for the limited purposes for which each user has given you permission to do so.[END_LEGAL_DOC]<|eot_id|><|start_header_id|>assistant<|end_header_id|>protect users sensitive data and have an adequate privacy notice.<|eot_id|><|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "print(create_prompt(legal_dataset[\"test\"][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mERZFXCdFLgI"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      - prompt: str representing formatted prompt\n",
        "      - model: model object\n",
        "      - tokenizer: tokenizer object\n",
        "\n",
        "    Functionality:\n",
        "      This will allow our model to generate a response to a prompt!\n",
        "\n",
        "    Returns:\n",
        "      - str response of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # convert str input into tokenized input\n",
        "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # send the tokenized inputs to our GPU\n",
        "    model_inputs = encoded_input.to(\"cuda\")\n",
        "\n",
        "    # generate response and set desired generation parameters\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # decode output from tokenized output to str output\n",
        "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "    # return only the generated response (not the prompt) as output\n",
        "    return decoded_output[0].split(\"<|end_header_id|>\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IQLq-a5XnMZg"
      },
      "outputs": [],
      "source": [
        "def generate_response(\n",
        "    prompt: str,\n",
        "    model: transformers.AutoModelForCausalLM,\n",
        "    tokenizer: transformers.AutoTokenizer,\n",
        ") -> str:\n",
        "    # convert str input into tokenized input\n",
        "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # send the tokenized inputs to our GPU\n",
        "    model_inputs = encoded_input.to(\"cuda\")\n",
        "\n",
        "    # generate response and set desired generation parameters\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # decode output from tokenized output to str output\n",
        "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "    # return only the generated response (not the prompt) as output\n",
        "    return decoded_output[0].split(\"<|end_header_id|>\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2JUb6X9UFPbs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "bca0142f-f4c6-4ad6-a27d-c3d2794d710a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nHere is a short human-readable summary:\\n\\n**Protecting User Privacy and Rights**\\n\\nIf you use our SDK to develop an app, you must:\\n\\n* Respect users' privacy and legal rights\\n* Clearly inform users when collecting their personal info (like names, passwords, or login details)\\n* Provide adequate protection for users' personal info, especially if you store it securely\\n* Only use Google account info with users' permission, and only for the specific purposes they've allowed.<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "generate_response(\n",
        "    create_prompt(legal_dataset[\"test\"][1], include_response=False), model, tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RezcTeu7Ql1J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30ec620c-4738-4696-ffd7-e2a83201e950"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'protect users sensitive data and have an adequate privacy notice.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Ground Truth Summary\n",
        "legal_dataset[\"test\"][1][\"reference_summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "894V7SAaevsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "d2417c66-fe8b-4c8d-9bf1-de203949efef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nHere is a short human-readable summary:\\n\\n**Community Guidelines**\\n\\nTo ensure a positive experience for everyone, please follow these guidelines when using our game:\\n\\n* Be respectful and considerate in your content.\\n* Avoid sharing content that is:\\n\\t+ Racist or discriminatory\\n\\t+ Bullying or harassing\\n\\t+ Damaging to our reputation or others\\n\\t+ Pornographic or advertising someone else's content\\n\\t+ Impersonating a moderator or trying to trick or exploit others\\n\\nBy following these guidelines, you can help create a safe and enjoyable community for everyone.<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Let's try another just to see how the model responds to a different prompt.\n",
        "generate_response(\n",
        "    create_prompt(legal_dataset[\"test\"][3], include_response=False), model, tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TDr329g8eyr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9249ee82-a7a7-49ca-a7b9-c8880625cab9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'things you make available on or through the game must not be offensive to people or illegal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Ground Truth Summary\n",
        "legal_dataset[\"test\"][3][\"reference_summary\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXNW9sbI_1u_"
      },
      "source": [
        "### Required Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "klHIwFqgG85s"
      },
      "outputs": [],
      "source": [
        "model_config = model.config\n",
        "model = peft.prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qk0mSs_7E2"
      },
      "source": [
        "## Task #3: Setting up PEFT LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5cVJcDWYKG0h"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model: transformers.AutoModelForCausalLM) -> None:\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        \"trainable params: {0:,} || all params: {1:,} || trainable%: {2:,}\".format(\n",
        "            trainable_params, all_param, 100 * trainable_params / all_param\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N11Mu9RAEVO"
      },
      "source": [
        "#### Initializing LoRA Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCJ9XrmsJJTZ"
      },
      "source": [
        "https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig\n",
        "https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9pr7p_mPHmML"
      },
      "outputs": [],
      "source": [
        "# set our rank (higher value is more memory/better performance)\n",
        "lora_r = 8\n",
        "\n",
        "# set our dropout (default value)\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# rule of thumb: alpha should be (lora_r * 2)\n",
        "lora_alpha = 16\n",
        "\n",
        "# construct our LoraConfig with the above hyperparameters\n",
        "peft_config = peft.LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = peft.get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-BrOR1FBcbG",
        "outputId": "aa00641d-d3bf-4abd-d6ee-b9106bf6eed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,971,520 || all params: 4,561,571,840 || trainable%: 0.4597432800707574\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwm5wUpN9TJm",
        "outputId": "cf8d415b-3041-46f5-c515-db2aceb10681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(128256, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaSdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (rotary_emb): LlamaRotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrBzhBTGHGLy"
      },
      "source": [
        "## Task #4: Training the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWtLUFNY7XDC"
      },
      "source": [
        "### Setting up Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "P0aN7bYf_bLj"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2_048\n",
        "\n",
        "args = trl.SFTConfig(\n",
        "    output_dir=\"llama381binstruct_summarize_short\",\n",
        "    # max_steps=500,\n",
        "    max_steps=100,\n",
        "    per_device_train_batch_size=1,\n",
        "    warmup_steps=30,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    },\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCCwez1-ZFoX"
      },
      "source": [
        "* https://huggingface.co/docs/trl/en/sft_trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_prompt_list(sample: str, include_response: bool = True) -> str:\n",
        "    full_prompt = INSTRUCTION_PROMPT_TEMPLATE.format(LEGAL_TEXT=sample[\"original_text\"])\n",
        "\n",
        "    if include_response:\n",
        "        full_prompt += RESPONSE_TEMPLATE.format(\n",
        "            NATURAL_LANGUAGE_SUMMARY=sample[\"reference_summary\"]\n",
        "        )\n",
        "\n",
        "    return [full_prompt]"
      ],
      "metadata": {
        "id": "Bjjp-iteka-X"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RZNQhfj6APJn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "db5f019d41aa4b76a59fb2bd80f02329",
            "74bbf7711e8042d9b5bd6a0ffc844399",
            "a383c6ad26e448299b4760f27a075817",
            "81b5269a227e4ca0b29d82487551eb85",
            "4e53057b7658437cb19efd0401331ddb",
            "a60dbed6b7fb43d7acc6e41815583848",
            "a89ad462038d497ea82cdbc906eaf269",
            "f918f970bd2743d8880bf9b01d1ad944",
            "9b67aa10bd374cf698a694ba50dfac67",
            "8299ce2ae5ae4f2daab6af8ad04477ec",
            "388b45130bcb418cbd4e885f870b021e",
            "2fdd2447628b4548abb24edb963c90c6",
            "1410ffc0a5eb47459f7b2492ba305434",
            "fb9dec89f3e34caab817ea2c5f3b733a",
            "09433026e70b42c38e67aa4fe0790b0f",
            "b3da291bca184b628619a58ec398cad0",
            "b3189b3cf19c429b9a77e0ea1d3be4a9",
            "c32d85ef92984097b594908efdb9c4a7",
            "2cd46c2d80da49bba722aa3e1b092885",
            "d08fa309419f45cabdbce6765860773c",
            "fa87704b5b7e4bae85dce036d550b5c2",
            "be9c44c3765544a7a4cfcd160b3ad2ef"
          ]
        },
        "outputId": "6f75b813-1408-4717-eec9-3ed1b050f530"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/68 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db5f019d41aa4b76a59fb2bd80f02329"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fdd2447628b4548abb24edb963c90c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "args.packing = False\n",
        "args.report_to = []\n",
        "\n",
        "trainer = trl.SFTTrainer(\n",
        "    model=model,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=_create_prompt_list,\n",
        "    args=args,\n",
        "    train_dataset=legal_dataset[\"train\"],\n",
        "    eval_dataset=legal_dataset[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "08UmcWvRjmAS",
        "outputId": "d439b31d-add3-49f0-efc7-487c6f3c1894"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 17:15, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>3.816771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.955279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.949226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.940927</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.00010582007234916091, metrics={'train_runtime': 1045.8615, 'train_samples_per_second': 0.096, 'train_steps_per_second': 0.096, 'total_flos': 9247821240729600.0, 'train_loss': 0.00010582007234916091, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYYoxEoEOPDY"
      },
      "source": [
        "## Task #5: Share Your Model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkD5hMhLMSzZ"
      },
      "outputs": [],
      "source": [
        "huggingface_hub.notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UOvMTDXMX28"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\"SethWeidman/llama381binstruct_summarize_short_merged_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAudsV-O4nT"
      },
      "source": [
        "### Compare Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKXqRmOLMdYD"
      },
      "outputs": [],
      "source": [
        "merged_model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6ulAvzY4tFG"
      },
      "outputs": [],
      "source": [
        "merged_model.push_to_hub(\n",
        "    \"SethWeidman/llama381binstruct_summarize_short_merged_test\",\n",
        "    safe_serialization=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwGvR26s-ExZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"SethWeidman/llama381binstruct_summarize_short_merged_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mikYCAawMf7p"
      },
      "outputs": [],
      "source": [
        "generate_response(\n",
        "    create_prompt(legal_dataset[\"test\"][1], include_response=False),\n",
        "    merged_model,\n",
        "    tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIyf0Cie6BwA"
      },
      "outputs": [],
      "source": [
        "legal_dataset[\"test\"][3][\"original_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQuDEmNGdCFi"
      },
      "outputs": [],
      "source": [
        "generate_response(\n",
        "    create_prompt(legal_dataset[\"test\"][3], include_response=False),\n",
        "    merged_model,\n",
        "    tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO4uEcuHfhWH"
      },
      "outputs": [],
      "source": [
        "# Ground Truth Summary\n",
        "legal_dataset[\"test\"][3][\"reference_summary\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db5f019d41aa4b76a59fb2bd80f02329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74bbf7711e8042d9b5bd6a0ffc844399",
              "IPY_MODEL_a383c6ad26e448299b4760f27a075817",
              "IPY_MODEL_81b5269a227e4ca0b29d82487551eb85"
            ],
            "layout": "IPY_MODEL_4e53057b7658437cb19efd0401331ddb"
          }
        },
        "74bbf7711e8042d9b5bd6a0ffc844399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a60dbed6b7fb43d7acc6e41815583848",
            "placeholder": "",
            "style": "IPY_MODEL_a89ad462038d497ea82cdbc906eaf269",
            "value": "Map:100%"
          }
        },
        "a383c6ad26e448299b4760f27a075817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f918f970bd2743d8880bf9b01d1ad944",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b67aa10bd374cf698a694ba50dfac67",
            "value": 68
          }
        },
        "81b5269a227e4ca0b29d82487551eb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8299ce2ae5ae4f2daab6af8ad04477ec",
            "placeholder": "",
            "style": "IPY_MODEL_388b45130bcb418cbd4e885f870b021e",
            "value": "68/68[00:00&lt;00:00,757.88examples/s]"
          }
        },
        "4e53057b7658437cb19efd0401331ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60dbed6b7fb43d7acc6e41815583848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89ad462038d497ea82cdbc906eaf269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f918f970bd2743d8880bf9b01d1ad944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b67aa10bd374cf698a694ba50dfac67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8299ce2ae5ae4f2daab6af8ad04477ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388b45130bcb418cbd4e885f870b021e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fdd2447628b4548abb24edb963c90c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1410ffc0a5eb47459f7b2492ba305434",
              "IPY_MODEL_fb9dec89f3e34caab817ea2c5f3b733a",
              "IPY_MODEL_09433026e70b42c38e67aa4fe0790b0f"
            ],
            "layout": "IPY_MODEL_b3da291bca184b628619a58ec398cad0"
          }
        },
        "1410ffc0a5eb47459f7b2492ba305434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3189b3cf19c429b9a77e0ea1d3be4a9",
            "placeholder": "",
            "style": "IPY_MODEL_c32d85ef92984097b594908efdb9c4a7",
            "value": "Map:100%"
          }
        },
        "fb9dec89f3e34caab817ea2c5f3b733a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cd46c2d80da49bba722aa3e1b092885",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d08fa309419f45cabdbce6765860773c",
            "value": 8
          }
        },
        "09433026e70b42c38e67aa4fe0790b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa87704b5b7e4bae85dce036d550b5c2",
            "placeholder": "",
            "style": "IPY_MODEL_be9c44c3765544a7a4cfcd160b3ad2ef",
            "value": "8/8[00:00&lt;00:00,306.53examples/s]"
          }
        },
        "b3da291bca184b628619a58ec398cad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3189b3cf19c429b9a77e0ea1d3be4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32d85ef92984097b594908efdb9c4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cd46c2d80da49bba722aa3e1b092885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d08fa309419f45cabdbce6765860773c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa87704b5b7e4bae85dce036d550b5c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9c44c3765544a7a4cfcd160b3ad2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}